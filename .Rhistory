library(car)
library(MASS)
library(nnet)
library(car)
library(ggpolt2)
library(ggplot2)
diamonds
head(diamonds)
ggplot(diamonds,aes=(x=diamonds$price))+geom_point()
?diamonds
library(datasets)
factor(diamonds$cut)
library(MASS)
library(nnet)
library(car)
library(ggplot2)
head(diamonds)
ggplot(diamonds,aes=(x=diamonds$price))+geom_point()
ggplot(diamonds,aes=(x=diamonds$price, y=diamonds$carat))+geom_point()
ggplot(diamonds,aes=(x=diamonds$price,y=diamonds$carat))+geom_point()
ggplot(diamonds,aes=(x=diamonds$price,y=diamonds$carat))+geom_point()
ggplot(diamonds,aes(x=diamonds$price,y=diamonds$carat))+geom_point()
ggplot(diamonds,aes(x=diamonds$price,y=diamonds$x))+geom_point()
?diamonds
lm.diamonds <- lm(price~carat,data=diamonds)
summary(lm.diamonds)
lm.diamonds <- lm(price~carat+depth,data=diamonds)
summary(lm.diamonds)
confit(lm.diamonds, level=0.95)
confint(lm.diamonds, level=0.95)
library(datasets)
#Ein Datensatz mit Geschwindigkeit (in mph) u. Bremsweg (in feet)
head(cars)
nrow(cars)
#Umrechnen in km/h u. meter u. id-Spalte hinzufuegen
mycars <- cars
mycars$dist <- round(mycars$dist*0.3048,digits=2)
mycars$speed <- round(mycars$speed*1.609344, digits=2)
mycars <- cbind(1:nrow(mycars),mycars)
colnames(mycars)[1] <- "id"
colnames(mycars)[2] <- "geschwindigkeit"
colnames(mycars)[3] <- "bremsweg"
head(mycars)
#Ziel: Vorhersage des Bremswegs aus der Geschwindigkeit
#Das ist, wie wir alle wissen, natürlich physikalische rSchwachfug
#da wir ja bereits eine exakte Berechnungsformel haben #ABER: das Ziel ist an einem nachvollziehbaren Beispiel #das grundsätzliche Vorgehen zu zeigen.
#SCHRITT 1: lineares Modell ohne Transformation #der Variablen bilden
lm.cars <- lm(bremsweg~geschwindigkeit, data=mycars)
round(lm.cars$coefficients, digits=2)
#Mit einem Scatterplot einen ersten Eindruck verschaffen
#NB: bei multipler Regression unmöglich
library(ggplot2)
ggplot(mycars, aes(x=geschwindigkeit, y=bremsweg)) +geom_point(shape=1)+ geom_abline(intercept=-5.36, slope=0.74, colour="green")+ labs(x="Geschwindigkeit in km/h",y="Bremsweg inm",title="Zusammenhang zw. Geschwindigkeit u. Bremsweg (smoothed; inkl. 95% Konfidenzintervall)")
#INTERPRETATION:
#linear sollte möglich,
#aber Transformation evtl. nötig
########################
#SCHRITT 2: GESAMTGUETE############ ########################
summary(lm.cars)
#R-square u. adjusted R-square:
# 64% der Varianz in Geschwindigkeit werden durch # Bremsweg erklärt
#F-Test wird hoechstsignifikant,
#d.h. die Nullhypothese, dass alle
#Regressionskoeffizienten in der Grundsgesamtheit gleich 0 sind #muss verworfen werden
#Residual standard error:
#Standardfehler der Schaetzung = 5.079, d.h. #bezogen auf die mittlere Geschwindigkeit beträgt der Standardfehler
#der Schaetzung 20% => ganz passabel
###################################### #SCHRITT 3: REGRESSIONSKOEFFIZIENTEN # ######################################
summary(lm.diamonds)
lm.diamonds$df.residual
lm.diamonds$qr
lm.diamonds$coefficients
summary(lm.diamonds)
1542/mean(diamonds$price)
confint(lm.diamonds, level=0.95)
library(gvlma)
gvmodel <- gvlma(lm.diamonds)
summary(gvmodel)
ncvTest(lm.diamonds)
spreadLevelPlot(lm.diamonds)
installed.packages()
install.packages
install.packages("pgirmess")
installed.packages()
install.packages("ez")
install.packages("DTK")
summary(gvmodel)
summary(lm.diamonds)
confint(lm.diamonds, level=0.95)
gvmodel <- gvlma(lm.diamonds)
summary(gvmodel)
plot(lm.diamonds)
opar <- par(no.readonly=FALSE)
par(mfrow=c(2,2))
plot(lm.diamonds)
par <- opar
ncvTest(lm.diamonds)
lm.diamonds <- lm(price~carat-depth,data=diamonds)
summary(lm.diamonds)
ggplot(diamonds,aes(x=diamonds$price,y=diamonds$depth))+geom_point()
ggplot(diamonds,aes(x=diamonds$price,y=diamonds$z))+geom_point()
ggplot(diamonds,aes(x=diamonds$price,y=diamonds$y))+geom_point()
ggplot(diamonds,aes(x=diamonds$price,y=diamonds$x))+geom_point()
ggplot(diamonds,aes(x=diamonds$price,y=diamonds$depth))+geom_point()
lm.diamonds <- lm(price~carat,data=diamonds)
summary(lm.diamonds)
confint(lm.diamonds, level=0.95)
gvmodel <- gvlma(lm.diamonds)
summary(gvmodel)
opar <- par(no.readonly=FALSE)
par(mfrow=c(2,2))
plot(lm.diamonds)
par <- opar
ncvTest(lm.diamonds)
spreadLevelPlot(lm.diamonds)
?diamonds
lm.diamonds <- lm(price~carat+depth,data=diamonds)
summary(lm.diamonds)
1542/mean(diamonds$price)
confint(lm.diamonds, level=0.95)
library(gvlma)
gvmodel <- gvlma(lm.diamonds)
summary(gvmodel)
opar <- par(no.readonly=FALSE)
par(mfrow=c(2,2))
plot(lm.diamonds)
par <- opar
#Homoskedaszität
ncvTest(lm.diamonds)
read.table("/Users/wolfganggross/Dropbox/Uni/WS 2013:14/Inf Datenanalyse/10_Praxiskurs/krebs.csv")
krebs <- read.table("/Users/wolfganggross/Dropbox/Uni/WS 2013:14/Inf Datenanalyse/10_Praxiskurs/krebs.csv",sep=",")
krebs
krebs <- read.table("/Users/wolfganggross/Dropbox/Uni/WS 2013:14/Inf Datenanalyse/10_Praxiskurs/krebs.csv",sep=",",header=TRUE)
krebs
library(reshape)
installed.packages()
library(reshape2)
library(reshape)
melt(krebs,var_name=Type)
subset(melt(krebs,var_name="Type")
subset(melt(krebs,var_name="Type")
subset(melt(krebs,var_name="Type")
melt(krebs,var_name="Type")
?melt
?melt()
subset(melt(krebs),melt(krebs)$value!=NA)
kr <- melt(krebs)
subset(kr,kr$value!=NA)
subset(kr,kr$value==NA)
kr[value==NA]
kr[kr$value==NA]
kr[value==NA]
kr$value[NA]
kr$value[!NA]
kr$value[!=NA]
kr
na.omit(kr)
kr<- na.omit(kr)
isfactor(kr$variable)
is.factor(kr$variable)
aov.kr  <- aov(value~variable,data=kr)
summary(aov.kr)
library(ggplot2)
geom_boxplot(kr,aes=(x=variable,y=value))
geom_boxplot(kr,aes(x=variable,y=value))
library(ggplot2)
ggplot(kr,aes(x=variable,y=value))+geom_boxplot()
library(DTK)
install.packages("rjags")
library(rjags)
# Goal: To make a panel of pictures.
par(mfrow=c(3,2))                       # 3 rows, 2 columns.
# Now the next 6 pictures will be placed on these 6 regions. :-)
# Let me take some pains on the 1st
plot(density(runif(100)), lwd=2)
text(x=0, y=0.2, "100 uniforms")        # Showing you how to place text at will
abline(h=0, v=0)
# All these statements effect the 1st plot.
x=seq(0.01,1,0.01)
par(col="blue")                         # default colour to blue.
# 2 --
plot(x, sin(x), type="l")
lines(x, cos(x), type="l", col="red")
# 3 --
plot(x, exp(x), type="l", col="green")
lines(x, log(x), type="l", col="orange")
# 4 --
plot(x, tan(x), type="l", lwd=3, col="yellow")
# 5 --
plot(x, exp(-x), lwd=2)
lines(x, exp(x), col="green", lwd=3)
# 6 --
plot(x, sin(x*x), type="l")
lines(x, sin(1/x), col="pink")
library("rjags")
library("rjags")
installed.packages()
library("rjags")
library("car")
library("ggplot")
install.packages("car")
library("ggplot")
install.packages("ggplot")
install.packages("ggplot2")
library("ggplot2")
library("rjags")
install.packages("rjags")
library("rjags")
library("rjags")
#################################################
source('~/Downloads/506_lecture1/506_lecture1.r', echo=TRUE)
#################################################
# POLS 506: Bayesian Statistics
# Dr. Justin Esarey
# Lecture 1: Conducting a Monte Carlo Assessment
#################################################
#####################################################################
# We will start with an introduction to the basic functions of R.
# NOTE: code preceded by a number sign (#) will not be executed by R
#####################################################################
# How to load data and run basic statistical models
rm(list=ls())       # clear out anything in memory
library(foreign)    # this loads functions that let you read Stata data sets
# Set the working directory of your R session
setwd("C:/jesarey_documents/POLS 506 - Topics in Methodology I/Lecture 1 - Monte Carlo Assessment")
?read.dta    # what does this function do?
my.data.set<-read.dta(file="506_lecture1.dta")  # read.dta() loads the Stata data set, called lecture0.dta
# now we have my.data.set in memory... let's look at it
View(my.data.set)     # View() opens a spreadsheet that lets you see your data frame
# variables in your data set live inside of your data frame.
happiness       # doesn't work!
my.data.set$happiness         # now it works
# if you don't want to reference the data frame every time, you can "attach" it to R's memory.
attach(my.data.set)
happiness
# now, let's do some analysis. What's the relationship between happiness and graduate stipend?
?plot
plot(happiness~stipend)
plot(happiness~stipend, main="Graduate Students: Happiness vs. Stipend", xlab="stipend (in thousands of $)", ylab="happiness (percentile)")
legend("topleft", legend=c("student"), pch=c(1))
# What's the relationship between happiness and year in school?
plot(happiness~years, main="Graduate Students: Happiness vs. Time", xlab="# of years in graduate school", ylab="happiness (percentile)")
# run a (somewhat inappropriate!) linear model relating happiness to stipend and years in school
?lm
linear.model<-lm(happiness~years+stipend)
# show the results
summary(linear.model)
# get the coefficients and VCV matrix
coefficients(linear.model)
vcov(linear.model)
# residuals and predicted values
resid<-residuals(linear.model)
pred<-predict(linear.model)
plot(resid~pred)
plot(resid~years)
# create a dichotomous variable called "happy"
# =1 if happiness > 0.5, =0 otherwise
detach(my.data.set)                                       # detach the data set
my.data.set$happy<-my.data.set$happiness>0.5              # add a variable that meets the specification
View(my.data.set)
my.data.set$happy<-as.numeric(my.data.set$happy)          # change T/F to numeric
View(my.data.set)
attach(my.data.set)
plot(happy~stipend)
plot(happy~years)
# run a logit model
?glm
logit.model<-glm(happy~years+stipend, family=binomial(link="logit"))
summary(logit.model)
# figure out marginal effect of moving years from 0 to 1,
# holding stipend at mean
mean.stipend<-mean(stipend)
predict.data<-data.frame(years=c(2,3), stipend=rep(mean.stipend,2))
change<-predict(logit.model, newdata=predict.data, type="response")
change[2]-change[1]
#
get the coefficients and VCV matrix
coefficients(logit.model)
vcov(logit.model)
# how about a probit?
probit.model<-glm(happy~years+stipend, family=binomial(link="probit"))
summary(probit.model)
# clear everything out again
detach(my.data.set)
rm(list=ls())
##############################
# Basic programming features:
# Scalars and Vectors
##############################
# create a scalar
a<-5
# display it (you can also see it in the "Workspace" pane)
a
# create a vector
b<-c(1,2,3,4,5)     # c() means "concatenate"
# What's the product of a*b?
a*b
# what's the product of two vectors?
d<-c(2,3,2,1,2)
b*d
# what's the sum/difference of two vectors?
b+d
b-d
# What's the third element of b?
b[3]
b[c(1,5)]
b[2:4]
d[2:4]
##############################
# Basic programming features:
# Matrices
#####################
# enter the matrix called A
A<-matrix(c(1,2,3,4,5,6), nrow=3, ncol=3)
# show that matrix
A
# note the difference if we tell to enter by row
AA<-matrix(c(1,2,3,4,5,6), nrow=3, ncol=3, byrow=T)
AA
# element-by-element sum and product of matrices
AA+A
AA*A
# "true" matrix multiplication
AA%*%A
# enter a simple vector x
x<-c(1,2,3,4,5,6)
# change the vector into a matrix
AA<-matrix(x, nrow=3, ncol=3, byrow=T)
AA
# R thinks that a row vector is DIFFERNT than a 1xk matrix
AA<-matrix(x, nrow=1, byrow=T)
AA
is.matrix(x)
is.matrix(AA)
# extract the first row vector
A[1,]
# extract the first column vector
A[,1]
# extract the middle element
A[2,2]
# extract a sub-matrix from matrix A, call it B
B<-A[2:3,2:3]
B
D<-matrix(c(1,2,6,2,4,2), ncol=1)
AA*D  # doesn't work
AA%*%D  # 1x1 scalar
D%*%AA  # 6x6 matrix
##############################
# Basic programming features:
# Arrays
#####################
X<-array(data=runif(36), dim=c(3,3,4))
X
X[1,1,3]
X[3,1,1]
X[,,2]
##############################
# Basic programming features:
# Lists
#####################
Y<-list(one=X[,,1], two=X[,,2], three=X[,,3])
Y$one
Y$two
Y<-list(one=X[,,1], two=X[,,2], three=c("convergence achieved"))
##############################
# Basic programming features:
# Random Variable Generators
#####################
# the uniform
?runif
runif(100)
runif(100, min=-10, max=10)
dunif(5, min=-10, max=10)
# the normal
?rnorm
rnorm(100)
rnorm(100, mean=-2, sd=3)
pnorm(-1.96)
# the multivariate normal
library(mvtnorm)
X<-rmvnorm(1000, mean=c(0,1), sigma=matrix(data=c(1,0,0,1), ncol=2, nrow=2, byrow=T))
plot(X[,1], X[,2])
##############################
# Basic programming features:
# Control Conditions
###############
# for loops
# the fibonacci sequence
a<-c(1,1)
for(i in 3:50){
a[i]<-a[i-1]+a[i-2]
}
# if conditions
# dichotomize a variable
a<-runif(100, min=-5, max=5)  # draw from the uniform distribution
b<-c()                        # empty vector
for(i in 1:length(a)){
if(a[i]>0){
b[i]<-1
}else{
b[i]<-0
}
}
# ...or, you could just do...
bb<-c()
bb<-as.numeric(a>0)
##############################
# Basic programming features:
# Functions
###############
myfunction<-function(a,b){
out<-a^b
return(out)
}
myfunction(2,2)
myfunction.two<-function(A){
out<-A[1]^A[2]
return(out)
}
myfunction.two(c(2,2))
# apply (like a for loop)
?apply
sample.mat<-matrix(data=runif(100), nrow=50, ncol=2)
apply(X=sample.mat, MARGIN=1, FUN=myfunction.two)
store<-c()
for(i in 1:dim(sample.mat)[1]){
store[i]<-myfunction.two(sample.mat[i,])
}
store
#######################################
# Conducting a Monte Carlo study
#######################################
# Question: how well does a LPM approximate a probit DGP?
# set parameters
N<- 100 # data set size
b.int <- 0 # intercept of probit DGP
b.x <- 1 # coefficient on IV
reps <- 1000 # number of times to repeat the simulation
probit.bias<-matrix(data=NA, nrow=reps, ncol=11) # matrix to store bias in probit predictions
linear.bias<-matrix(data=NA, nrow=reps, ncol=11) # matrix to store bias in linear model predictions
pb<-txtProgressBar(initial=0, min=1, max=reps, style=3) # progress bar
for(i in 1:reps){
setTxtProgressBar(pb, value=i)  # update the progress bar
# create a probit data set
x<-runif(N)
u<-rnorm(N, mean=0, sd=1)
ystar<-b.int+b.x*x+u
y<-ifelse(ystar>0, 1, 0)
# values of x over which to predict the data
pred.dat<-data.frame(x=seq(from=0, to=1, by=0.1))
# probit model
probit.mod<-glm(y~x, family=binomial(link="probit"))
# linear model
lin.mod<-lm(y~x)
# predict Pr(y=1) using both models
probit.pred<-predict(probit.mod, type="response", newdata=pred.dat)
lin.pred<-predict(lin.mod, newdata=pred.dat)
# true values
true.pred<-pnorm(b.int+b.x*pred.dat$x, mean=0, sd=1)
# calculate and store bias
probit.bias[i,]<-probit.pred-true.pred
linear.bias[i,]<-lin.pred-true.pred
}
close(pb)   # stop the progress bar
boxplot(probit.bias)
boxplot(linear.bias)
boxplot(probit.bias-linear.bias)
library(ggplot2)
dim(probit.bias)<-c(reps*11, 1)
dim(linear.bias)<-c(reps*11, 1)
bias.dat<-data.frame(bias=rbind(probit.bias, linear.bias), x=factor(rep(seq(from=0, to=1, by=0.1), each=1000), levels=seq(from=0, to=1, by=0.1)), model=factor(rep(c("probit", "linear"), each=reps*11), levels=c("probit", "linear")))
qplot(x, bias, facets=model~., geom="boxplot", data=bias.dat)
qplot(model, bias, facets=~x, geom="boxplot", data=bias.dat)
rm(list=ls())       # clear out anything in memory
library(foreign)    # this loads functions that let you read Stata data sets
?read.dta    # what does this function do?
my.data.set<-read.dta(file="506_lecture1.dta")  # read.dta() loads the Stata data set, called lecture0.dta
#example http://www.johnmyleswhite.com/notebook/2010/08/20/using-jags-in-r-with-the-rjags-package/
library("rjags")
N <- 1000
x <- rnorm(N, 0, 5)
write.table(x,
file = 'example1.data',
row.names = FALSE,
col.names = FALSE)
col.names = FALSE)
col.names = FALSE)
write.table(x,file = 'example1.data',row.names = FALSE,col.names = FALSE)
dat <- read.csv('/Volumes/Macintosh HD/Users/wolfganggross/Desktop/newfile.csv')
dat <- read.csv('/Volumes/Macintosh HD/Users/wolfganggross/Desktop/newfile.csv')
dat <- read.csv('/Volumes/Macintosh HD/Users/wolfganggross/Desktop/newfile1.csv')s
dat <- read.csv('/Volumes/Macintosh HD/Users/wolfganggross/Desktop/newfile1.csv')
dat <- read.csv('/Volumes/Macintosh HD/Users/wolfganggross/Desktop/newfile1.csv',header=false,sep=',')
dat <- read.csv('/Volumes/Macintosh HD/Users/wolfganggross/Desktop/newfile1.csv',header=false,sep=',')
dat <- read.csv('/Volumes/Macintosh HD/Users/wolfganggross/Desktop/newfile1.csv')
View(dat)
View(dat)
libury(ggplot2)
libary(ggplot2)
library(ggplot2)
qplot(dat)
View(dat)
installed.packages
installed.packages()
install.packages('shiny')
defaults write org.R-project.R force.LANG en_US.UTF-8
install.packages('shiny')
system('locale')
system('locale')
install.packages('shiny')
install.packages('shiny')
setwd("~/Documents/programmierung/Social_Data_Mining")
